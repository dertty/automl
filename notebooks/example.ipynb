{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# libs\n",
    "Аналоги:  \n",
    "- https://mlbox.readthedocs.io/en/latest/  \n",
    "- https://lightautoml.readthedocs.io/  \n",
    "- https://automl.github.io/auto-sklearn/master/  \n",
    "\n",
    "Tюнинг:\n",
    "- https://optuna.org/\n",
    "- https://hyperopt.github.io/hyperopt/\n",
    "- https://docs.ray.io/en/latest/tune/index.html \n",
    "- https://oss-vizier.readthedocs.io/en/latest/ \n",
    "\n",
    "Работа с фичами:\n",
    "- https://scikit-learn.org/1.5/modules/feature_selection.html\n",
    "- https://feature-engine.trainindata.com/en/1.7.x/api_doc/selection/index.html\n",
    "- https://github.com/AutoViML/featurewiz\n",
    "- https://github.com/scikit-learn-contrib/boruta_py\n",
    "\n",
    "Прочее:\n",
    "- https://unit8co.github.io/darts/\n",
    "- https://epistasislab.github.io/tpot/  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# datasets\n",
    "Сберовский датасет:\n",
    "- https://huggingface.co/datasets/ai-lab/MBD (14,6 GB, основной датасет)\n",
    "- https://huggingface.co/datasets/ai-lab/MBD-mini (3,38 GB, уменьшенная версия, 10% клиентов из основного датасета)  \n",
    "\n",
    "Kaggle datasets:\n",
    "- https://www.kaggle.com/datasets/yasserh/titanic-dataset\n",
    "- https://www.kaggle.com/datasets/rabieelkharoua/predict-liver-disease-1700-records-dataset\n",
    "- https://www.kaggle.com/datasets/devzohaib/eligibility-prediction-for-loan\n",
    "- https://www.kaggle.com/datasets/deepcontractor/smoke-detection-dataset\n",
    "- https://www.kaggle.com/datasets/stealthtechnologies/employee-attrition-dataset\n",
    "- https://www.kaggle.com/datasets/rabieelkharoua/cancer-prediction-dataset\n",
    "- https://www.kaggle.com/datasets/yasserh/heart-disease-dataset\n",
    "- https://www.kaggle.com/datasets/rabieelkharoua/predict-online-course-engagement-dataset\n",
    "- https://www.kaggle.com/datasets/rabieelkharoua/predicting-hiring-decisions-in-recruitment-data\n",
    "- https://www.kaggle.com/datasets/barun2104/telecom-churn\n",
    "- https://www.kaggle.com/datasets/rabieelkharoua/predicting-manufacturing-defects-dataset\n",
    "- https://www.kaggle.com/datasets/rabieelkharoua/predict-customer-purchase-behavior-dataset\n",
    "- https://www.kaggle.com/datasets/teejmahal20/airline-passenger-satisfaction\n",
    "- https://www.kaggle.com/datasets/dhanushnarayananr/credit-card-fraud\n",
    "- https://www.kaggle.com/datasets/rameshmehta/credit-risk-analysis\n",
    "- https://www.kaggle.com/datasets/marcpaulo/titanic-huge-dataset-1m-passengers\n",
    "- https://www.kaggle.com/datasets/manishtripathi86/fedex-data\n",
    "- https://www.kaggle.com/datasets/ban7002/fraud-challenge-data\n",
    "- https://www.kaggle.com/datasets/ulrikthygepedersen/kickstarter-projects\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "\n",
    "import kaggle\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "kaggle.api.authenticate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [\n",
    "    {\n",
    "        'link': 'https://www.kaggle.com/datasets/yasserh/titanic-dataset', \n",
    "        'target': 'Survived',\n",
    "        'id_cols': ['PassengerId',],\n",
    "    },\n",
    "    {\n",
    "        'link': 'https://www.kaggle.com/datasets/rabieelkharoua/predict-liver-disease-1700-records-dataset', \n",
    "        'target': 'Diagnosis',\n",
    "    },\n",
    "    {\n",
    "        'link': 'https://www.kaggle.com/datasets/devzohaib/eligibility-prediction-for-loan', \n",
    "        'target': 'Loan_Status',\n",
    "        'id_cols': ['Loan_ID',],\n",
    "    },\n",
    "    {\n",
    "        'link': 'https://www.kaggle.com/datasets/deepcontractor/smoke-detection-dataset', \n",
    "        'target': 'Fire Alarm',\n",
    "        'id_cols': ['index',],\n",
    "    },\n",
    "    {\n",
    "        'link': 'https://www.kaggle.com/datasets/stealthtechnologies/employee-attrition-dataset', \n",
    "        'target': 'Attrition',\n",
    "        'id_cols': ['Employee ID',],\n",
    "    },\n",
    "    {\n",
    "        'link': 'https://www.kaggle.com/datasets/rabieelkharoua/cancer-prediction-dataset', \n",
    "        'target': 'Diagnosis',\n",
    "    },\n",
    "    {\n",
    "        'link': 'https://www.kaggle.com/datasets/yasserh/heart-disease-dataset', \n",
    "        'target': 'target',\n",
    "    },\n",
    "    {\n",
    "        'link': 'https://www.kaggle.com/datasets/rabieelkharoua/predict-online-course-engagement-dataset', \n",
    "        'target': 'CourseCompletion',\n",
    "        'id_cols': ['UserID',],\n",
    "    },\n",
    "    {\n",
    "        'link': 'https://www.kaggle.com/datasets/rabieelkharoua/predicting-hiring-decisions-in-recruitment-data', \n",
    "        'target': 'HiringDecision',\n",
    "    },\n",
    "    {\n",
    "        'link': 'https://www.kaggle.com/datasets/barun2104/telecom-churn', \n",
    "        'target': 'Churn',\n",
    "    },\n",
    "    {\n",
    "        'link': 'https://www.kaggle.com/datasets/rabieelkharoua/predicting-manufacturing-defects-dataset', \n",
    "        'target': 'DefectStatus',\n",
    "    },\n",
    "    {\n",
    "        'link': 'https://www.kaggle.com/datasets/rabieelkharoua/predict-customer-purchase-behavior-dataset',\n",
    "        'target': 'PurchaseStatus',\n",
    "    },\n",
    "    {\n",
    "        'link': 'https://www.kaggle.com/datasets/teejmahal20/airline-passenger-satisfaction', \n",
    "        'target': 'Satisfaction',\n",
    "    },\n",
    "    {\n",
    "        'link': 'https://www.kaggle.com/datasets/dhanushnarayananr/credit-card-fraud', \n",
    "        'target': 'fraud',\n",
    "    },\n",
    "    {\n",
    "        'link': 'https://www.kaggle.com/datasets/rameshmehta/credit-risk-analysis', \n",
    "        'target': 'default_ind',\n",
    "        'id_cols': ['id', 'member_id',],\n",
    "        'time_col': 'issue_d',\n",
    "    },\n",
    "    {\n",
    "        'link': 'https://www.kaggle.com/datasets/marcpaulo/titanic-huge-dataset-1m-passengers', \n",
    "        'target': 'Survived',\n",
    "        'id_cols': ['PassengerId',],\n",
    "    },\n",
    "    # {\n",
    "    #     'link': 'https://www.kaggle.com/datasets/manishtripathi86/fedex-data', \n",
    "    #     'target': 'Delivery_Status',\n",
    "    #     'time_col': 'Year',\n",
    "    # },\n",
    "    {\n",
    "        'link': 'https://www.kaggle.com/datasets/ban7002/fraud-challenge-data', \n",
    "        'target': 'EVENT_LABEL',\n",
    "        'time_col': 'EVENT_TIMESTAMP',\n",
    "    },\n",
    "    {\n",
    "        'link': 'https://www.kaggle.com/datasets/ulrikthygepedersen/kickstarter-projects', \n",
    "        'target': 'State',\n",
    "        'id_cols': ['ID',],\n",
    "        'time_col': 'Launched',\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in datasets:\n",
    "    dataset_name = dataset['link'].replace('https://www.kaggle.com/datasets/', '')\n",
    "    dataset_path = Path(f'./datasets/{dataset_name}')\n",
    "    if not dataset_path.exists():\n",
    "        if len(dataset_path.glob('*.csv')) == 0:\n",
    "            kaggle.api.dataset_download_files(dataset_name, path=dataset_path, unzip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_test_datasets(datasets, datasets_path, test_size=0.3, random_state=0):\n",
    "    from tqdm import tqdm\n",
    "    from pathlib import Path\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    \n",
    "    \n",
    "    for dataset in tqdm(datasets):\n",
    "        dataset_name = dataset['link'].replace('https://www.kaggle.com/datasets/', '')\n",
    "        files = list((Path(datasets_path) / dataset_name).glob('*.csv'))\n",
    "        if len(files) == 1:\n",
    "            dataset_path = files[0]\n",
    "            data = pd.read_csv(dataset_path)\n",
    "            target_name = dataset['target']\n",
    "            data = data.dropna(subset=[target_name])\n",
    "            \n",
    "            if dataset.get('id_cols'):\n",
    "                data = data.drop(columns=[dataset['id_cols']], errors='ignore')\n",
    "\n",
    "            le = LabelEncoder()\n",
    "            data[target_name] = le.fit_transform(data[target_name])\n",
    "            data = data.select_dtypes(include=[np.number]).dropna(how='all', axis='columns').fillna(0)\n",
    "            train_data, test_data = train_test_split(\n",
    "                data, \n",
    "                test_size=test_size, \n",
    "                stratify=data[target_name], \n",
    "                random_state=random_state\n",
    "            )\n",
    "            yield train_data, test_data, target_name, dataset_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Модуль model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sklearn metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from automl.model import AutoML\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "results = []\n",
    "for train, test, target_name, dataset_name in get_train_test_datasets(datasets[:1], datasets_path='./datasets', test_size=0.3, random_state=0):\n",
    "    start_time = time.perf_counter()\n",
    "    automl = AutoML(task='classification', n_jobs=12, metric='roc_auc', tuning_timeout=30,)\n",
    "    automl = automl.fit(\n",
    "        train.drop(columns=[target_name,]), train[target_name].to_numpy(), \n",
    "        test.drop(columns=[target_name,]), test[target_name].to_numpy(),\n",
    "    )\n",
    "    train_time = time.perf_counter() - start_time\n",
    "    \n",
    "    test_predictions = automl.predict(test.drop(columns=[target_name,]))\n",
    "    predict_time = time.perf_counter() - start_time - train_time\n",
    "    all_time = train_time + predict_time\n",
    "    \n",
    "    test_score = roc_auc_score(test[target_name], test_predictions[:, 1])\n",
    "    results.append((dataset_name, test_score, automl.best_model.name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('yasserh/titanic-dataset', 0.7724036481318035, 'TabularLama')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## custom metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-11-02 14:28:34]\n",
      "[2024-11-02 14:28:35,129] - [   MODEL    ] - 1 out of 15. LogisticRegression\n",
      "[2024-11-02 14:28:35,130] - [   START    ] - Working with LogisticRegression\n",
      "[2024-11-02 14:28:35,131] - [   START    ] - Tuning LogisticRegression\n",
      "[2024-11-02 14:29:08,179] - [BEST PARAMS ] - {'C': 0.005994842503189409, 'class_weight': 'balanced', 'max_iter': 1000, 'n_jobs': 12, 'random_state': 42}\n",
      "[2024-11-02 14:29:08,181] - [    END     ] - Tuning LogisticRegression\n",
      "[2024-11-02 14:29:08,182] - [   START    ] - Fitting LogisticRegression\n",
      "[2024-11-02 14:29:08,184] - [    FIT     ] - LogisticRegression fold 0\n",
      "[2024-11-02 14:29:13,439] - [    FIT     ] - LogisticRegression fold 1\n",
      "[2024-11-02 14:29:18,671] - [    FIT     ] - LogisticRegression fold 2\n",
      "[2024-11-02 14:29:18,687] - [    FIT     ] - LogisticRegression fold 3\n",
      "[2024-11-02 14:29:18,702] - [    FIT     ] - LogisticRegression fold 4\n",
      "[2024-11-02 14:29:18,719] - [    END     ] - Fitting LogisticRegression\n",
      "[2024-11-02 14:29:18,723] - [   SCORE    ] - Train: 0.7290141213389122\n",
      "[2024-11-02 14:29:18,725] - [   SCORE    ] - OOF: 0.7178674163179917\n",
      "[2024-11-02 14:29:18,729] - [   SCORE    ] - Test: 0.7026772580170638\n",
      "[2024-11-02 14:29:18,729] - [   SCORE    ] - Overfit: 3.61 %\n",
      "[2024-11-02 14:29:18,740] - [    END     ] - Working with LogisticRegression\n",
      "[2024-11-02 14:29:18,741] - [  NEW BEST  ] - LogisticRegression. Best score: 0.7026772580170638 \n",
      "\n",
      "[2024-11-02 14:29:18,742] - [   MODEL    ] - 2 out of 15. RandomForestClassification\n",
      "[2024-11-02 14:29:18,744] - [   START    ] - Working with RandomForestClassification\n",
      "[2024-11-02 14:29:18,744] - [   START    ] - Tuning RandomForestClassification\n",
      "[2024-11-02 14:29:20,868] - [   OPTUNA   ] - Trial 0. New best score 0.7113727864147796 with parameters {'n_estimators': 387, 'max_depth': 15, 'min_samples_split': 0.146398788362281, 'min_samples_leaf': 0.11973169683940732, 'max_features': 0.24041677639819287, 'criterion': 'log_loss', 'class_weight': 'balanced_subsample', 'oob_score': False, 'max_samples': 0.8341182143924175}\n",
      "[2024-11-02 14:29:21,984] - [   OPTUNA   ] - Trial 1. New best score 0.7325454442440724 with parameters {'n_estimators': 228, 'max_depth': 4, 'min_samples_split': 0.03668090197068676, 'min_samples_leaf': 0.06084844859190755, 'max_features': 0.5722807884690141, 'criterion': 'log_loss', 'class_weight': 'balanced_subsample', 'oob_score': False, 'max_samples': 0.7873242017790835}\n",
      "[2024-11-02 14:29:23,330] - [   OPTUNA   ] - Trial 2. New best score 0.7434504570177862 with parameters {'n_estimators': 215, 'max_depth': 9, 'min_samples_split': 0.1184829137724085, 'min_samples_leaf': 0.009290082543999545, 'max_features': 0.6467903667112945, 'criterion': 'log_loss', 'class_weight': 'balanced', 'oob_score': True, 'max_samples': 0.6873906962470353}\n",
      "[2024-11-02 14:29:25,229] - [   OPTUNA   ] - Trial 3. New best score 0.7442467992999908 with parameters {'n_estimators': 451, 'max_depth': 3, 'min_samples_split': 0.09903538202225404, 'min_samples_leaf': 0.006877704223043679, 'max_features': 0.9183883618709039, 'criterion': 'entropy', 'class_weight': 'balanced_subsample', 'oob_score': False, 'max_samples': 0.7773814951275034}\n",
      "[2024-11-02 14:29:48,925] - [   OPTUNA   ] - 15 trials completed\n",
      "[2024-11-02 14:29:48,927] - [BEST PARAMS ] - {'n_estimators': 451, 'criterion': 'entropy', 'max_depth': 3, 'min_samples_split': 0.09903538202225404, 'min_samples_leaf': 0.006877704223043679, 'max_features': 0.9183883618709039, 'bootstrap': True, 'oob_score': False, 'max_samples': 0.7773814951275034, 'n_jobs': 12, 'random_state': 42, 'class_weight': 'balanced_subsample'}\n",
      "[2024-11-02 14:29:48,928] - [    END     ] - Tuning RandomForestClassification\n",
      "[2024-11-02 14:29:48,929] - [   START    ] - Fitting RandomForestClassification\n",
      "[2024-11-02 14:29:48,931] - [    FIT     ] - RandomForestClassification fold 0\n",
      "[2024-11-02 14:29:50,541] - [    FIT     ] - RandomForestClassification fold 1\n",
      "[2024-11-02 14:29:52,146] - [    FIT     ] - RandomForestClassification fold 2\n",
      "[2024-11-02 14:29:53,764] - [    FIT     ] - RandomForestClassification fold 3\n",
      "[2024-11-02 14:29:55,373] - [    FIT     ] - RandomForestClassification fold 4\n",
      "[2024-11-02 14:29:56,976] - [    END     ] - Fitting RandomForestClassification\n",
      "[2024-11-02 14:29:58,010] - [   SCORE    ] - Train: 0.789400278940028\n",
      "[2024-11-02 14:29:58,013] - [   SCORE    ] - OOF: 0.7398012552301255\n",
      "[2024-11-02 14:29:59,024] - [   SCORE    ] - Test: 0.7285083848190644\n",
      "[2024-11-02 14:29:59,026] - [   SCORE    ] - Overfit: 7.71 %\n",
      "[2024-11-02 14:29:59,865] - [    END     ] - Working with RandomForestClassification\n",
      "[2024-11-02 14:29:59,867] - [  NEW BEST  ] - RandomForestClassification. Best score: 0.7285083848190644 \n",
      "\n",
      "[2024-11-02 14:29:59,867] - [   MODEL    ] - 3 out of 15. ExtraTreesClassification\n",
      "[2024-11-02 14:29:59,868] - [   START    ] - Working with ExtraTreesClassification\n",
      "[2024-11-02 14:29:59,869] - [   START    ] - Tuning ExtraTreesClassification\n",
      "[2024-11-02 14:30:01,362] - [   OPTUNA   ] - Trial 0. New best score 0.7060003051026988 with parameters {'n_estimators': 387, 'max_depth': 15, 'min_samples_split': 0.146398788362281, 'min_samples_leaf': 0.11973169683940732, 'max_features': 0.24041677639819287, 'criterion': 'log_loss', 'class_weight': 'balanced_subsample', 'oob_score': False, 'max_samples': 0.8341182143924175}\n",
      "[2024-11-02 14:30:02,491] - [   OPTUNA   ] - Trial 1. New best score 0.724697639286805 with parameters {'n_estimators': 228, 'max_depth': 4, 'min_samples_split': 0.03668090197068676, 'min_samples_leaf': 0.06084844859190755, 'max_features': 0.5722807884690141, 'criterion': 'log_loss', 'class_weight': 'balanced_subsample', 'oob_score': False, 'max_samples': 0.7873242017790835}\n",
      "[2024-11-02 14:30:03,832] - [   OPTUNA   ] - Trial 2. New best score 0.7346565761599354 with parameters {'n_estimators': 215, 'max_depth': 9, 'min_samples_split': 0.1184829137724085, 'min_samples_leaf': 0.009290082543999545, 'max_features': 0.6467903667112945, 'criterion': 'log_loss', 'class_weight': 'balanced', 'oob_score': True, 'max_samples': 0.6873906962470353}\n",
      "[2024-11-02 14:30:05,708] - [   OPTUNA   ] - Trial 3. New best score 0.7404277588313143 with parameters {'n_estimators': 451, 'max_depth': 3, 'min_samples_split': 0.09903538202225404, 'min_samples_leaf': 0.006877704223043679, 'max_features': 0.9183883618709039, 'criterion': 'entropy', 'class_weight': 'balanced_subsample', 'oob_score': False, 'max_samples': 0.7773814951275034}\n",
      "[2024-11-02 14:30:25,759] - [   OPTUNA   ] - Trial 11. New best score 0.7440408171056268 with parameters {'n_estimators': 32, 'max_depth': 10, 'min_samples_split': 0.07121824466699872, 'min_samples_leaf': 0.004689401807340943, 'max_features': 0.7134393876140748, 'criterion': 'entropy', 'class_weight': 'balanced_subsample', 'oob_score': True, 'max_samples': 0.6405508745066752}\n",
      "[2024-11-02 14:30:26,379] - [   OPTUNA   ] - Trial 13. New best score 0.7468522081921438 with parameters {'n_estimators': 36, 'max_depth': 7, 'min_samples_split': 0.054273833433723856, 'min_samples_leaf': 0.0003893728893889538, 'max_features': 0.8460708041987088, 'criterion': 'entropy', 'class_weight': 'balanced_subsample', 'oob_score': False, 'max_samples': 0.36629114865941276}\n",
      "[2024-11-02 14:30:29,903] - [   OPTUNA   ] - 18 trials completed\n",
      "[2024-11-02 14:30:29,905] - [BEST PARAMS ] - {'n_estimators': 36, 'criterion': 'entropy', 'max_depth': 7, 'min_samples_split': 0.054273833433723856, 'min_samples_leaf': 0.0003893728893889538, 'max_features': 0.8460708041987088, 'bootstrap': True, 'oob_score': False, 'max_samples': 0.36629114865941276, 'n_jobs': 12, 'random_state': 42, 'class_weight': 'balanced_subsample', 'verbose': 0}\n",
      "[2024-11-02 14:30:29,906] - [    END     ] - Tuning ExtraTreesClassification\n",
      "[2024-11-02 14:30:29,907] - [   START    ] - Fitting ExtraTreesClassification\n",
      "[2024-11-02 14:30:29,910] - [    FIT     ] - ExtraTreesClassification fold 0\n",
      "[2024-11-02 14:30:30,046] - [    FIT     ] - ExtraTreesClassification fold 1\n",
      "[2024-11-02 14:30:30,188] - [    FIT     ] - ExtraTreesClassification fold 2\n",
      "[2024-11-02 14:30:30,329] - [    FIT     ] - ExtraTreesClassification fold 3\n",
      "[2024-11-02 14:30:30,472] - [    FIT     ] - ExtraTreesClassification fold 4\n",
      "[2024-11-02 14:30:30,612] - [    END     ] - Fitting ExtraTreesClassification\n",
      "[2024-11-02 14:30:30,720] - [   SCORE    ] - Train: 0.7865128138075315\n",
      "[2024-11-02 14:30:30,723] - [   SCORE    ] - OOF: 0.7450858612273361\n",
      "[2024-11-02 14:30:30,831] - [   SCORE    ] - Test: 0.7270962047661077\n",
      "[2024-11-02 14:30:30,833] - [   SCORE    ] - Overfit: 7.55 %\n",
      "[2024-11-02 14:30:30,935] - [    END     ] - Working with ExtraTreesClassification\n",
      "[2024-11-02 14:30:30,936] - [BEST  MODEL ] - RandomForestClassification. Best score: 0.7285083848190644 \n",
      "\n",
      "[2024-11-02 14:30:30,937] - [   MODEL    ] - 4 out of 15. CatBoostClassification\n",
      "[2024-11-02 14:30:30,938] - [   START    ] - Working with CatBoostClassification\n",
      "[2024-11-02 14:30:30,939] - [   START    ] - Tuning CatBoostClassification\n",
      "[2024-11-02 14:30:33,129] - [   OPTUNA   ] - Trial 0. New best score 0.752439761153367 with parameters {'boosting_type': 'Plain', 'depth': 6, 'l2_leaf_reg': 190.14286128198324, 'bootstrap_type': 'Bernoulli', 'grow_policy': 'SymmetricTree', 'min_data_in_leaf': 222, 'rsm': 0.7606690070459252, 'subsample': 0.8248435466776274, 'model_size_reg': 4.116898859160489, 'auto_class_weights': None, 'iterations': 451}\n",
      "[2024-11-02 14:30:34,056] - [   OPTUNA   ] - Trial 1. New best score 0.7531820211796416 with parameters {'boosting_type': 'Plain', 'depth': 3, 'l2_leaf_reg': 36.68090197068676, 'bootstrap_type': 'MVS', 'grow_policy': 'Lossguide', 'min_data_in_leaf': 36, 'rsm': 0.5752867891211308, 'subsample': 0.619817105976215, 'model_size_reg': 91.21399684340719, 'auto_class_weights': None, 'max_leaves': 307, 'iterations': 249}\n",
      "[2024-11-02 14:30:38,055] - [   OPTUNA   ] - Trial 5. New best score 0.7571838354348153 with parameters {'boosting_type': 'Plain', 'depth': 4, 'l2_leaf_reg': 1.1044234247204798, 'bootstrap_type': 'Bernoulli', 'grow_policy': 'Depthwise', 'min_data_in_leaf': 92, 'rsm': 0.4695214357150779, 'subsample': 0.9178620555253562, 'model_size_reg': 124.65962536551159, 'auto_class_weights': None, 'iterations': 217}\n",
      "[2024-11-02 14:30:56,505] - [   OPTUNA   ] - Trial 11. New best score 0.7581797669948565 with parameters {'boosting_type': 'Plain', 'depth': 4, 'l2_leaf_reg': 1.681410764787914, 'bootstrap_type': 'MVS', 'grow_policy': 'Lossguide', 'min_data_in_leaf': 13, 'rsm': 0.5872672912553432, 'subsample': 0.6453850087358457, 'model_size_reg': 128.50188633556886, 'auto_class_weights': None, 'max_leaves': 455, 'iterations': 136}\n",
      "[2024-11-02 14:31:01,679] - [   OPTUNA   ] - 17 trials completed\n",
      "[2024-11-02 14:31:01,680] - [BEST PARAMS ] - {'boosting_type': 'Plain', 'iterations': 136, 'learning_rate': 0.03, 'max_leaves': 455, 'grow_policy': 'Lossguide', 'depth': 4, 'l2_leaf_reg': 1.681410764787914, 'model_size_reg': 128.50188633556886, 'od_wait': 100, 'bootstrap_type': 'MVS', 'rsm': 0.5872672912553432, 'subsample': 0.6453850087358457, 'min_data_in_leaf': 13, 'one_hot_max_size': 10, 'auto_class_weights': None, 'thread_count': 12, 'random_state': 42, 'verbose': False, 'allow_writing_files': False}\n",
      "[2024-11-02 14:31:01,681] - [    END     ] - Tuning CatBoostClassification\n",
      "[2024-11-02 14:31:01,682] - [   START    ] - Fitting CatBoostClassification\n",
      "[2024-11-02 14:31:01,683] - [    FIT     ] - CatBoostClassification fold 0\n",
      "[2024-11-02 14:31:01,753] - [    FIT     ] - CatBoostClassification fold 1\n",
      "[2024-11-02 14:31:01,821] - [    FIT     ] - CatBoostClassification fold 2\n",
      "[2024-11-02 14:31:01,896] - [    FIT     ] - CatBoostClassification fold 3\n",
      "[2024-11-02 14:31:01,966] - [    FIT     ] - CatBoostClassification fold 4\n",
      "[2024-11-02 14:31:02,040] - [    END     ] - Fitting CatBoostClassification\n",
      "[2024-11-02 14:31:02,051] - [   SCORE    ] - Train: 0.807226290097629\n",
      "[2024-11-02 14:31:02,054] - [   SCORE    ] - OOF: 0.7511658821478382\n",
      "[2024-11-02 14:31:02,060] - [   SCORE    ] - Test: 0.7473374521918211\n",
      "[2024-11-02 14:31:02,061] - [   SCORE    ] - Overfit: 7.42 %\n",
      "[2024-11-02 14:31:02,077] - [    END     ] - Working with CatBoostClassification\n",
      "[2024-11-02 14:31:02,078] - [  NEW BEST  ] - CatBoostClassification. Best score: 0.7473374521918211 \n",
      "\n",
      "[2024-11-02 14:31:02,079] - [   MODEL    ] - 5 out of 15. LightGBMClassification\n",
      "[2024-11-02 14:31:02,080] - [   START    ] - Working with LightGBMClassification\n",
      "[2024-11-02 14:31:02,082] - [   START    ] - Tuning LightGBMClassification\n",
      "[2024-11-02 14:31:02,302] - [   OPTUNA   ] - Trial 0. New best score 0.6982001970593796 with parameters {'max_depth': 6, 'num_leaves': 488, 'min_data_in_leaf': 188, 'bagging_fraction': 0.7993292420985183, 'bagging_freq': 0, 'feature_fraction': 0.49359671220172163, 'lambda_l1': 0.5808361216819946, 'lambda_l2': 8.661761457749352, 'min_gain_to_split': 12.022300234864176, 'is_unbalance': True, 'num_iterations': 51}\n",
      "[2024-11-02 14:31:02,437] - [   OPTUNA   ] - Trial 1. New best score 0.7199702593039661 with parameters {'max_depth': 16, 'num_leaves': 428, 'min_data_in_leaf': 55, 'bagging_fraction': 0.5909124836035503, 'bagging_freq': 0, 'feature_fraction': 0.5825453457757226, 'lambda_l1': 5.247564316322379, 'lambda_l2': 4.319450186421157, 'min_gain_to_split': 5.824582803960839, 'is_unbalance': True, 'num_iterations': 107}\n",
      "[2024-11-02 14:31:04,946] - [   OPTUNA   ] - Trial 22. New best score 0.7232352747802775 with parameters {'max_depth': 16, 'num_leaves': 385, 'min_data_in_leaf': 40, 'bagging_fraction': 0.5836527799752635, 'bagging_freq': 0, 'feature_fraction': 0.5938933529227833, 'lambda_l1': 5.134343034366853, 'lambda_l2': 5.480560145203352, 'min_gain_to_split': 2.521912125406444, 'is_unbalance': True, 'num_iterations': 72}\n",
      "[2024-11-02 14:31:05,124] - [   OPTUNA   ] - Trial 23. New best score 0.7255847019022411 with parameters {'max_depth': 16, 'num_leaves': 382, 'min_data_in_leaf': 36, 'bagging_fraction': 0.7507355705035088, 'bagging_freq': 0, 'feature_fraction': 0.5756685143491068, 'lambda_l1': 6.592743408020324, 'lambda_l2': 2.0345124556202423, 'min_gain_to_split': 2.0093484355986977, 'is_unbalance': True, 'num_iterations': 100}\n",
      "[2024-11-02 14:31:05,317] - [   OPTUNA   ] - Trial 24. New best score 0.7260594668489405 with parameters {'max_depth': 16, 'num_leaves': 378, 'min_data_in_leaf': 37, 'bagging_fraction': 0.7940264221036825, 'bagging_freq': 0, 'feature_fraction': 0.4598523850570049, 'lambda_l1': 9.061128996240324, 'lambda_l2': 1.9698373848952624, 'min_gain_to_split': 1.9195294834848409, 'is_unbalance': True, 'num_iterations': 123}\n",
      "[2024-11-02 14:31:06,723] - [   OPTUNA   ] - Trial 31. New best score 0.7287494303941673 with parameters {'max_depth': 13, 'num_leaves': 304, 'min_data_in_leaf': 93, 'bagging_fraction': 0.7922287095621834, 'bagging_freq': 0, 'feature_fraction': 0.45765528280503964, 'lambda_l1': 9.252663758480445, 'lambda_l2': 0.21891514820133207, 'min_gain_to_split': 0.0633940880880215, 'is_unbalance': True, 'num_iterations': 182}\n",
      "[2024-11-02 14:31:14,606] - [   OPTUNA   ] - Trial 74. New best score 0.7295946663685325 with parameters {'max_depth': 5, 'num_leaves': 427, 'min_data_in_leaf': 91, 'bagging_fraction': 0.6923828123678143, 'bagging_freq': 0, 'feature_fraction': 0.44837000052691506, 'lambda_l1': 3.140512246273379, 'lambda_l2': 3.2315044152132972, 'min_gain_to_split': 0.46260829411448035, 'is_unbalance': True, 'num_iterations': 178}\n",
      "[2024-11-02 14:31:14,961] - [   OPTUNA   ] - Trial 75. New best score 0.7327882841775619 with parameters {'max_depth': 5, 'num_leaves': 467, 'min_data_in_leaf': 111, 'bagging_fraction': 0.6357487970202809, 'bagging_freq': 0, 'feature_fraction': 0.4494132055890141, 'lambda_l1': 2.1626127714002794, 'lambda_l2': 3.247368994886184, 'min_gain_to_split': 0.01820970435551672, 'is_unbalance': True, 'num_iterations': 266}\n",
      "[2024-11-02 14:31:21,494] - [   OPTUNA   ] - Trial 105. New best score 0.7455114833141849 with parameters {'max_depth': 5, 'num_leaves': 486, 'min_data_in_leaf': 80, 'bagging_fraction': 0.6581843288738521, 'bagging_freq': 0, 'feature_fraction': 0.9795566966929125, 'lambda_l1': 0.9518001873141395, 'lambda_l2': 5.454838850849149, 'min_gain_to_split': 0.0053677913135053434, 'is_unbalance': True, 'num_iterations': 226}\n",
      "[2024-11-02 14:31:32,116] - [   OPTUNA   ] - 153 trials completed\n",
      "[2024-11-02 14:31:32,117] - [BEST PARAMS ] - {'objective_type': 'binary', 'boosting': 'gbdt', 'num_iterations': 226, 'max_depth': 5, 'learning_rate': 0.03, 'num_leaves': 486, 'min_data_in_leaf': 80, 'bagging_fraction': 0.6581843288738521, 'bagging_freq': 0, 'feature_fraction': 0.9795566966929125, 'early_stopping_round': 100, 'lambda_l1': 0.9518001873141395, 'lambda_l2': 5.454838850849149, 'min_gain_to_split': 0.0053677913135053434, 'num_threads': 12, 'random_state': 42, 'is_unbalance': True, 'num_classes': 1, 'verbose': -1}\n",
      "[2024-11-02 14:31:32,118] - [    END     ] - Tuning LightGBMClassification\n",
      "[2024-11-02 14:31:32,120] - [   START    ] - Fitting LightGBMClassification\n",
      "[2024-11-02 14:31:32,123] - [    FIT     ] - LightGBMClassification fold 0\n",
      "[2024-11-02 14:31:32,156] - [    FIT     ] - LightGBMClassification fold 1\n",
      "[2024-11-02 14:31:32,203] - [    FIT     ] - LightGBMClassification fold 2\n",
      "[2024-11-02 14:31:32,255] - [    FIT     ] - LightGBMClassification fold 3\n",
      "[2024-11-02 14:31:32,305] - [    FIT     ] - LightGBMClassification fold 4\n",
      "[2024-11-02 14:31:32,350] - [    END     ] - Fitting LightGBMClassification\n",
      "[2024-11-02 14:31:32,357] - [   SCORE    ] - Train: 0.7784769438633194\n",
      "[2024-11-02 14:31:32,359] - [   SCORE    ] - OOF: 0.7365269787308228\n",
      "[2024-11-02 14:31:32,364] - [   SCORE    ] - Test: 0.7076787290379524\n",
      "[2024-11-02 14:31:32,365] - [   SCORE    ] - Overfit: 9.09 %\n",
      "[2024-11-02 14:31:32,406] - [    END     ] - Working with LightGBMClassification\n",
      "[2024-11-02 14:31:32,407] - [BEST  MODEL ] - CatBoostClassification. Best score: 0.7473374521918211 \n",
      "\n",
      "[2024-11-02 14:31:32,408] - [   MODEL    ] - 6 out of 15. XGBClassification\n",
      "[2024-11-02 14:31:32,409] - [   START    ] - Working with XGBClassification\n",
      "[2024-11-02 14:31:32,411] - [   START    ] - Tuning XGBClassification\n",
      "[2024-11-02 14:31:33,694] - [   OPTUNA   ] - Trial 0. New best score 0.7068292482293741 with parameters {'max_depth': 6, 'grow_policy': 'depthwise', 'max_leaves': 311, 'gamma': 3.1203728088487304, 'subsample': 0.2403950683025824, 'colsample_bytree': 0.15227525095137953, 'colsample_bylevel': 0.8795585311974417, 'reg_lambda': 6.011150117432088, 'reg_alpha': 7.080725777960454, 'min_child_weight': 0, 'class_weight': None, 'n_estimators': 456}\n",
      "[2024-11-02 14:31:34,810] - [   OPTUNA   ] - Trial 1. New best score 0.7140008356238761 with parameters {'max_depth': 4, 'grow_policy': 'lossguide', 'max_leaves': 163, 'gamma': 10.495128632644757, 'subsample': 0.48875051677790415, 'colsample_bytree': 0.36210622617823773, 'colsample_bylevel': 0.6506676052501416, 'reg_lambda': 1.3949386065204183, 'reg_alpha': 2.9214464853521815, 'min_child_weight': 7, 'class_weight': 'balanced', 'n_estimators': 542}\n",
      "[2024-11-02 14:31:40,999] - [   OPTUNA   ] - Trial 7. New best score 0.7212988043125221 with parameters {'max_depth': 12, 'grow_policy': 'depthwise', 'max_leaves': 397, 'gamma': 9.875911927287815, 'subsample': 0.5704595464437947, 'colsample_bytree': 0.4847869165226947, 'colsample_bylevel': 0.12287721406968567, 'reg_lambda': 1.0789142699330445, 'reg_alpha': 0.3142918568673425, 'min_child_weight': 13, 'class_weight': 'balanced', 'n_estimators': 330}\n",
      "[2024-11-02 14:31:45,162] - [   OPTUNA   ] - Trial 12. New best score 0.7235849915528667 with parameters {'max_depth': 12, 'grow_policy': 'depthwise', 'max_leaves': 184, 'gamma': 7.600699882199473, 'subsample': 0.9621756602495342, 'colsample_bytree': 0.6233913736674418, 'colsample_bylevel': 0.7847051653792725, 'reg_lambda': 2.3346917474433844, 'reg_alpha': 0.03438807836189428, 'min_child_weight': 9, 'class_weight': 'balanced', 'n_estimators': 243}\n",
      "[2024-11-02 14:31:49,969] - [   OPTUNA   ] - Trial 17. New best score 0.726594949353558 with parameters {'max_depth': 14, 'grow_policy': 'depthwise', 'max_leaves': 108, 'gamma': 4.675115502743981, 'subsample': 0.6166195985636427, 'colsample_bytree': 0.8014853820805108, 'colsample_bylevel': 0.5321558732515757, 'reg_lambda': 3.448421144907398, 'reg_alpha': 4.1022042258930975, 'min_child_weight': 20, 'class_weight': 'balanced', 'n_estimators': 552}\n",
      "[2024-11-02 14:31:58,740] - [   OPTUNA   ] - Trial 25. New best score 0.7312161720791339 with parameters {'max_depth': 14, 'grow_policy': 'depthwise', 'max_leaves': 252, 'gamma': 1.770731757803465, 'subsample': 0.7071324541737908, 'colsample_bytree': 0.961263319336994, 'colsample_bylevel': 0.711793904058992, 'reg_lambda': 6.882871480930099, 'reg_alpha': 3.974877366911932, 'min_child_weight': 19, 'class_weight': 'balanced', 'n_estimators': 458}\n",
      "[2024-11-02 14:32:03,591] - [   OPTUNA   ] - 30 trials completed\n",
      "[2024-11-02 14:32:03,592] - [BEST PARAMS ] - {'objective': 'binary:logistic', 'n_estimators': 458, 'learning_rate': 0.03, 'max_depth': 14, 'max_leaves': 252, 'grow_policy': 'depthwise', 'gamma': 1.770731757803465, 'min_child_weight': 19, 'subsample': 0.7071324541737908, 'colsample_bytree': 0.961263319336994, 'colsample_bylevel': 0.711793904058992, 'reg_lambda': 6.882871480930099, 'reg_alpha': 3.974877366911932, 'enable_categorical': True, 'max_cat_to_onehot': 5, 'n_jobs': 12, 'random_state': 42, 'verbosity': 0, 'early_stopping_rounds': 100, 'class_weight': 'balanced'}\n",
      "[2024-11-02 14:32:03,593] - [    END     ] - Tuning XGBClassification\n",
      "[2024-11-02 14:32:03,595] - [   START    ] - Fitting XGBClassification\n",
      "[2024-11-02 14:32:03,597] - [    FIT     ] - XGBClassification fold 0\n",
      "[2024-11-02 14:32:03,733] - [    FIT     ] - XGBClassification fold 1\n",
      "[2024-11-02 14:32:03,932] - [    FIT     ] - XGBClassification fold 2\n",
      "[2024-11-02 14:32:04,099] - [    FIT     ] - XGBClassification fold 3\n",
      "[2024-11-02 14:32:04,275] - [    FIT     ] - XGBClassification fold 4\n",
      "[2024-11-02 14:32:04,492] - [    END     ] - Fitting XGBClassification\n",
      "[2024-11-02 14:32:04,507] - [   SCORE    ] - Train: 0.750032688284519\n",
      "[2024-11-02 14:32:04,510] - [   SCORE    ] - OOF: 0.7263663702928871\n",
      "[2024-11-02 14:32:04,524] - [   SCORE    ] - Test: 0.7042365401588703\n",
      "[2024-11-02 14:32:04,525] - [   SCORE    ] - Overfit: 6.11 %\n",
      "[2024-11-02 14:32:04,561] - [    END     ] - Working with XGBClassification\n",
      "[2024-11-02 14:32:04,562] - [BEST  MODEL ] - CatBoostClassification. Best score: 0.7473374521918211 \n",
      "\n",
      "[2024-11-02 14:32:04,562] - [   MODEL    ] - 7 out of 15. TabularLama\n",
      "[2024-11-02 14:32:04,564] - [   START    ] - Working with TabularLama\n",
      "[2024-11-02 14:32:04,564] - [   START    ] - Fitting TabularLama\n",
      "[14:32:04] Stdout logging level is INFO.\n",
      "[14:32:04] Copying TaskTimer may affect the parent PipelineTimer, so copy will create new unlimited TaskTimer\n",
      "[14:32:04] Task: binary\n",
      "\n",
      "[14:32:04] Start automl preset with listed constraints:\n",
      "[14:32:04] - time: 60.00 seconds\n",
      "[14:32:04] - CPU: 12 cores\n",
      "[14:32:04] - memory: 16 GB\n",
      "\n",
      "[14:32:04] \u001b[1mTrain data shape: (623, 7)\u001b[0m\n",
      "\n",
      "[14:32:04] Layer \u001b[1m1\u001b[0m train process start. Time left 59.90 secs\n",
      "[14:32:04] Start fitting \u001b[1mLvl_0_Pipe_0_Mod_0_LinearL2\u001b[0m ...\n",
      "[14:32:28] Time limit exceeded after calculating fold 0\n",
      "\n",
      "[14:32:28] Fitting \u001b[1mLvl_0_Pipe_0_Mod_0_LinearL2\u001b[0m finished. score = \u001b[1m0.698051948051948\u001b[0m\n",
      "[14:32:28] \u001b[1mLvl_0_Pipe_0_Mod_0_LinearL2\u001b[0m fitting and predicting completed\n",
      "[14:32:28] Time left 35.67 secs\n",
      "\n",
      "[14:32:29] \u001b[1mSelector_LightGBM\u001b[0m fitting and predicting completed\n",
      "[14:32:29] Start fitting \u001b[1mLvl_0_Pipe_1_Mod_0_LightGBM\u001b[0m ...\n",
      "[14:32:31] Time limit exceeded after calculating fold 3\n",
      "\n",
      "[14:32:31] Fitting \u001b[1mLvl_0_Pipe_1_Mod_0_LightGBM\u001b[0m finished. score = \u001b[1m0.7278931801183108\u001b[0m\n",
      "[14:32:31] \u001b[1mLvl_0_Pipe_1_Mod_0_LightGBM\u001b[0m fitting and predicting completed\n",
      "[14:32:31] Start hyperparameters optimization for \u001b[1mLvl_0_Pipe_1_Mod_1_Tuned_LightGBM\u001b[0m ... Time budget is 1.00 secs\n",
      "[14:32:33] Hyperparameters optimization for \u001b[1mLvl_0_Pipe_1_Mod_1_Tuned_LightGBM\u001b[0m completed\n",
      "[14:32:33] Start fitting \u001b[1mLvl_0_Pipe_1_Mod_1_Tuned_LightGBM\u001b[0m ...\n",
      "[14:32:34] Fitting \u001b[1mLvl_0_Pipe_1_Mod_1_Tuned_LightGBM\u001b[0m finished. score = \u001b[1m0.7439526673640167\u001b[0m\n",
      "[14:32:34] \u001b[1mLvl_0_Pipe_1_Mod_1_Tuned_LightGBM\u001b[0m fitting and predicting completed\n",
      "[14:32:34] Start fitting \u001b[1mLvl_0_Pipe_1_Mod_2_CatBoost\u001b[0m ...\n",
      "[14:32:35] Fitting \u001b[1mLvl_0_Pipe_1_Mod_2_CatBoost\u001b[0m finished. score = \u001b[1m0.7591091352859135\u001b[0m\n",
      "[14:32:35] \u001b[1mLvl_0_Pipe_1_Mod_2_CatBoost\u001b[0m fitting and predicting completed\n",
      "[14:32:35] Start hyperparameters optimization for \u001b[1mLvl_0_Pipe_1_Mod_3_Tuned_CatBoost\u001b[0m ... Time budget is 17.75 secs\n",
      "[14:32:48] Hyperparameters optimization for \u001b[1mLvl_0_Pipe_1_Mod_3_Tuned_CatBoost\u001b[0m completed\n",
      "[14:32:48] Start fitting \u001b[1mLvl_0_Pipe_1_Mod_3_Tuned_CatBoost\u001b[0m ...\n",
      "[14:32:49] Fitting \u001b[1mLvl_0_Pipe_1_Mod_3_Tuned_CatBoost\u001b[0m finished. score = \u001b[1m0.7678913877266387\u001b[0m\n",
      "[14:32:49] \u001b[1mLvl_0_Pipe_1_Mod_3_Tuned_CatBoost\u001b[0m fitting and predicting completed\n",
      "[14:32:49] Time left 15.09 secs\n",
      "\n",
      "[14:32:49] Time limit exceeded in one of the tasks. AutoML will blend level 1 models.\n",
      "\n",
      "[14:32:49] \u001b[1mLayer 1 training completed.\u001b[0m\n",
      "\n",
      "[14:32:49] Blending: optimization starts with equal weights and score \u001b[1m0.765483350767085\u001b[0m\n",
      "[14:32:49] Blending: iteration \u001b[1m0\u001b[0m: score = \u001b[1m0.7694931136680614\u001b[0m, weights = \u001b[1m[0.0655874  0.         0.11728158 0.16823123 0.64889973]\u001b[0m\n",
      "[14:32:49] Blending: iteration \u001b[1m1\u001b[0m: score = \u001b[1m0.7695584902370991\u001b[0m, weights = \u001b[1m[0.         0.         0.19367628 0.16600625 0.6403175 ]\u001b[0m\n",
      "[14:32:49] Blending: iteration \u001b[1m2\u001b[0m: score = \u001b[1m0.7696456589958158\u001b[0m, weights = \u001b[1m[0.         0.         0.14589804 0.17584285 0.67825913]\u001b[0m\n",
      "[14:32:49] Blending: iteration \u001b[1m3\u001b[0m: score = \u001b[1m0.7697764121338913\u001b[0m, weights = \u001b[1m[0.0513     0.         0.13841347 0.16682212 0.64346445]\u001b[0m\n",
      "[14:32:49] Blending: iteration \u001b[1m4\u001b[0m: score = \u001b[1m0.7697764121338913\u001b[0m, weights = \u001b[1m[0.0513     0.         0.13841346 0.16682212 0.6434644 ]\u001b[0m\n",
      "[14:32:49] Blending: no score update. Terminated\n",
      "\n",
      "[14:32:49] \u001b[1mAutoml preset training completed in 45.20 seconds\u001b[0m\n",
      "\n",
      "[14:32:49] Model description:\n",
      "Final prediction for new objects (level 0) = \n",
      "\t 0.05130 * (1 averaged models Lvl_0_Pipe_0_Mod_0_LinearL2) +\n",
      "\t 0.13841 * (5 averaged models Lvl_0_Pipe_1_Mod_1_Tuned_LightGBM) +\n",
      "\t 0.16682 * (5 averaged models Lvl_0_Pipe_1_Mod_2_CatBoost) +\n",
      "\t 0.64346 * (5 averaged models Lvl_0_Pipe_1_Mod_3_Tuned_CatBoost) \n",
      "\n",
      "[2024-11-02 14:32:49,809] - [    END     ] - Fitting TabularLama\n",
      "[2024-11-02 14:32:49,876] - [   SCORE    ] - Train: 0.8944931136680614\n",
      "[2024-11-02 14:32:49,878] - [   SCORE    ] - OOF: 0.7697764121338913\n",
      "[2024-11-02 14:32:49,939] - [   SCORE    ] - Test: 0.7724036481318035\n",
      "[2024-11-02 14:32:49,940] - [   SCORE    ] - Overfit: 13.65 %\n",
      "[2024-11-02 14:32:50,043] - [    END     ] - Working with TabularLama\n",
      "[2024-11-02 14:32:50,044] - [  NEW BEST  ] - TabularLama. Best score: 0.7724036481318035 \n",
      "\n",
      "[2024-11-02 14:32:50,045] - [   MODEL    ] - 8 out of 15. TabularLamaUtilized\n",
      "[2024-11-02 14:32:50,046] - [   START    ] - Working with TabularLamaUtilized\n",
      "[2024-11-02 14:32:50,047] - [   START    ] - Fitting TabularLamaUtilized\n",
      "[14:32:50] Start automl \u001b[1mutilizator\u001b[0m with listed constraints:\n",
      "[14:32:50] - time: 60.00 seconds\n",
      "[14:32:50] - CPU: 12 cores\n",
      "[14:32:50] - memory: 16 GB\n",
      "\n",
      "[14:32:50] \u001b[1mIf one preset completes earlier, next preset configuration will be started\u001b[0m\n",
      "\n",
      "[14:32:50] ==================================================\n",
      "[14:32:50] Start 0 automl preset configuration:\n",
      "[14:32:50] \u001b[1mconf_0_sel_type_0.yml\u001b[0m, random state: {'reader_params': {'random_state': 42}, 'nn_params': {'random_state': 42}, 'general_params': {'return_all_predictions': False}}\n",
      "[14:32:50] Stdout logging level is INFO.\n",
      "[14:32:50] Task: binary\n",
      "\n",
      "[14:32:50] Start automl preset with listed constraints:\n",
      "[14:32:50] - time: 60.00 seconds\n",
      "[14:32:50] - CPU: 12 cores\n",
      "[14:32:50] - memory: 16 GB\n",
      "\n",
      "[14:32:50] \u001b[1mTrain data shape: (623, 7)\u001b[0m\n",
      "\n",
      "[14:32:50] Layer \u001b[1m1\u001b[0m train process start. Time left 59.94 secs\n",
      "[14:32:50] Start fitting \u001b[1mLvl_0_Pipe_0_Mod_0_LinearL2\u001b[0m ...\n",
      "[14:32:51] Fitting \u001b[1mLvl_0_Pipe_0_Mod_0_LinearL2\u001b[0m finished. score = \u001b[1m0.7397358786610878\u001b[0m\n",
      "[14:32:51] \u001b[1mLvl_0_Pipe_0_Mod_0_LinearL2\u001b[0m fitting and predicting completed\n",
      "[14:32:51] Time left 58.67 secs\n",
      "\n",
      "[14:32:51] Start fitting \u001b[1mLvl_0_Pipe_1_Mod_0_LightGBM\u001b[0m ...\n",
      "[14:32:54] Fitting \u001b[1mLvl_0_Pipe_1_Mod_0_LightGBM\u001b[0m finished. score = \u001b[1m0.7356062587168759\u001b[0m\n",
      "[14:32:54] \u001b[1mLvl_0_Pipe_1_Mod_0_LightGBM\u001b[0m fitting and predicting completed\n",
      "[14:32:54] Start hyperparameters optimization for \u001b[1mLvl_0_Pipe_1_Mod_1_Tuned_LightGBM\u001b[0m ... Time budget is 1.00 secs\n",
      "[14:32:55] Hyperparameters optimization for \u001b[1mLvl_0_Pipe_1_Mod_1_Tuned_LightGBM\u001b[0m completed\n",
      "[14:32:55] Start fitting \u001b[1mLvl_0_Pipe_1_Mod_1_Tuned_LightGBM\u001b[0m ...\n",
      "[14:32:56] Fitting \u001b[1mLvl_0_Pipe_1_Mod_1_Tuned_LightGBM\u001b[0m finished. score = \u001b[1m0.7439526673640167\u001b[0m\n",
      "[14:32:56] \u001b[1mLvl_0_Pipe_1_Mod_1_Tuned_LightGBM\u001b[0m fitting and predicting completed\n",
      "[14:32:56] Start fitting \u001b[1mLvl_0_Pipe_1_Mod_2_CatBoost\u001b[0m ...\n",
      "[14:32:57] Fitting \u001b[1mLvl_0_Pipe_1_Mod_2_CatBoost\u001b[0m finished. score = \u001b[1m0.7591091352859135\u001b[0m\n",
      "[14:32:57] \u001b[1mLvl_0_Pipe_1_Mod_2_CatBoost\u001b[0m fitting and predicting completed\n",
      "[14:32:57] Start hyperparameters optimization for \u001b[1mLvl_0_Pipe_1_Mod_3_Tuned_CatBoost\u001b[0m ... Time budget is 44.38 secs\n",
      "[14:33:11] Hyperparameters optimization for \u001b[1mLvl_0_Pipe_1_Mod_3_Tuned_CatBoost\u001b[0m completed\n",
      "[14:33:11] Start fitting \u001b[1mLvl_0_Pipe_1_Mod_3_Tuned_CatBoost\u001b[0m ...\n",
      "[14:33:11] Fitting \u001b[1mLvl_0_Pipe_1_Mod_3_Tuned_CatBoost\u001b[0m finished. score = \u001b[1m0.7678913877266387\u001b[0m\n",
      "[14:33:11] \u001b[1mLvl_0_Pipe_1_Mod_3_Tuned_CatBoost\u001b[0m fitting and predicting completed\n",
      "[14:33:11] Time left 38.27 secs\n",
      "\n",
      "[14:33:11] \u001b[1mLayer 1 training completed.\u001b[0m\n",
      "\n",
      "[14:33:11] Blending: optimization starts with equal weights and score \u001b[1m0.766572960251046\u001b[0m\n",
      "[14:33:11] Blending: iteration \u001b[1m0\u001b[0m: score = \u001b[1m0.7707461645746165\u001b[0m, weights = \u001b[1m[0.11034714 0.11490422 0.12797783 0.13793184 0.50883895]\u001b[0m\n",
      "[14:33:11] Blending: iteration \u001b[1m1\u001b[0m: score = \u001b[1m0.7710403591352859\u001b[0m, weights = \u001b[1m[0.11540394 0.07110886 0.13707727 0.14425275 0.5321571 ]\u001b[0m\n",
      "[14:33:12] Blending: iteration \u001b[1m2\u001b[0m: score = \u001b[1m0.7711493200836821\u001b[0m, weights = \u001b[1m[0.13073882 0.06982139 0.1345954  0.1423223  0.5225221 ]\u001b[0m\n",
      "[14:33:12] Blending: iteration \u001b[1m3\u001b[0m: score = \u001b[1m0.7711493200836821\u001b[0m, weights = \u001b[1m[0.13073882 0.06982139 0.1345954  0.1423223  0.5225221 ]\u001b[0m\n",
      "[14:33:12] Blending: no score update. Terminated\n",
      "\n",
      "[14:33:12] \u001b[1mAutoml preset training completed in 21.97 seconds\u001b[0m\n",
      "\n",
      "[14:33:12] Model description:\n",
      "Final prediction for new objects (level 0) = \n",
      "\t 0.13074 * (5 averaged models Lvl_0_Pipe_0_Mod_0_LinearL2) +\n",
      "\t 0.06982 * (5 averaged models Lvl_0_Pipe_1_Mod_0_LightGBM) +\n",
      "\t 0.13460 * (5 averaged models Lvl_0_Pipe_1_Mod_1_Tuned_LightGBM) +\n",
      "\t 0.14232 * (5 averaged models Lvl_0_Pipe_1_Mod_2_CatBoost) +\n",
      "\t 0.52252 * (5 averaged models Lvl_0_Pipe_1_Mod_3_Tuned_CatBoost) \n",
      "\n",
      "[14:33:12] ==================================================\n",
      "[14:33:12] Start 1 automl preset configuration:\n",
      "[14:33:12] \u001b[1mconf_1_sel_type_1.yml\u001b[0m, random state: {'reader_params': {'random_state': 43}, 'nn_params': {'random_state': 43}, 'general_params': {'return_all_predictions': False}}\n",
      "[14:33:12] Stdout logging level is INFO.\n",
      "[14:33:12] Task: binary\n",
      "\n",
      "[14:33:12] Start automl preset with listed constraints:\n",
      "[14:33:12] - time: 38.00 seconds\n",
      "[14:33:12] - CPU: 12 cores\n",
      "[14:33:12] - memory: 16 GB\n",
      "\n",
      "[14:33:12] \u001b[1mTrain data shape: (623, 7)\u001b[0m\n",
      "\n",
      "[14:33:12] Layer \u001b[1m1\u001b[0m train process start. Time left 37.94 secs\n",
      "[14:33:12] Start fitting \u001b[1mLvl_0_Pipe_0_Mod_0_LinearL2\u001b[0m ...\n",
      "[14:33:12] Fitting \u001b[1mLvl_0_Pipe_0_Mod_0_LinearL2\u001b[0m finished. score = \u001b[1m0.701675819386332\u001b[0m\n",
      "[14:33:12] \u001b[1mLvl_0_Pipe_0_Mod_0_LinearL2\u001b[0m fitting and predicting completed\n",
      "[14:33:12] Time left 37.27 secs\n",
      "\n",
      "[14:33:13] \u001b[1mSelector_LightGBM\u001b[0m fitting and predicting completed\n",
      "[14:33:13] Start fitting \u001b[1mLvl_0_Pipe_1_Mod_0_LightGBM\u001b[0m ...\n",
      "[14:33:16] Time limit exceeded after calculating fold 3\n",
      "\n",
      "[14:33:16] Fitting \u001b[1mLvl_0_Pipe_1_Mod_0_LightGBM\u001b[0m finished. score = \u001b[1m0.723465016658734\u001b[0m\n",
      "[14:33:16] \u001b[1mLvl_0_Pipe_1_Mod_0_LightGBM\u001b[0m fitting and predicting completed\n",
      "[14:33:16] Start hyperparameters optimization for \u001b[1mLvl_0_Pipe_1_Mod_1_Tuned_LightGBM\u001b[0m ... Time budget is 1.00 secs\n",
      "[14:33:18] Hyperparameters optimization for \u001b[1mLvl_0_Pipe_1_Mod_1_Tuned_LightGBM\u001b[0m completed\n",
      "[14:33:18] Start fitting \u001b[1mLvl_0_Pipe_1_Mod_1_Tuned_LightGBM\u001b[0m ...\n",
      "[14:33:19] Fitting \u001b[1mLvl_0_Pipe_1_Mod_1_Tuned_LightGBM\u001b[0m finished. score = \u001b[1m0.7477172681311017\u001b[0m\n",
      "[14:33:19] \u001b[1mLvl_0_Pipe_1_Mod_1_Tuned_LightGBM\u001b[0m fitting and predicting completed\n",
      "[14:33:19] Start fitting \u001b[1mLvl_0_Pipe_1_Mod_2_CatBoost\u001b[0m ...\n",
      "[14:33:20] Fitting \u001b[1mLvl_0_Pipe_1_Mod_2_CatBoost\u001b[0m finished. score = \u001b[1m0.7537428085774058\u001b[0m\n",
      "[14:33:20] \u001b[1mLvl_0_Pipe_1_Mod_2_CatBoost\u001b[0m fitting and predicting completed\n",
      "[14:33:20] Start hyperparameters optimization for \u001b[1mLvl_0_Pipe_1_Mod_3_Tuned_CatBoost\u001b[0m ... Time budget is 18.86 secs\n",
      "[14:33:35] Hyperparameters optimization for \u001b[1mLvl_0_Pipe_1_Mod_3_Tuned_CatBoost\u001b[0m completed\n",
      "[14:33:35] Start fitting \u001b[1mLvl_0_Pipe_1_Mod_3_Tuned_CatBoost\u001b[0m ...\n",
      "[14:33:36] Fitting \u001b[1mLvl_0_Pipe_1_Mod_3_Tuned_CatBoost\u001b[0m finished. score = \u001b[1m0.7538953539051604\u001b[0m\n",
      "[14:33:36] \u001b[1mLvl_0_Pipe_1_Mod_3_Tuned_CatBoost\u001b[0m fitting and predicting completed\n",
      "[14:33:36] Time left 13.68 secs\n",
      "\n",
      "[14:33:36] Time limit exceeded in one of the tasks. AutoML will blend level 1 models.\n",
      "\n",
      "[14:33:36] \u001b[1mLayer 1 training completed.\u001b[0m\n",
      "\n",
      "[14:33:36] Blending: optimization starts with equal weights and score \u001b[1m0.7649712343096233\u001b[0m\n",
      "[14:33:36] Blending: iteration \u001b[1m0\u001b[0m: score = \u001b[1m0.7682291666666666\u001b[0m, weights = \u001b[1m[0.08034004 0.         0.43109447 0.17651805 0.31204745]\u001b[0m\n",
      "[14:33:36] Blending: iteration \u001b[1m1\u001b[0m: score = \u001b[1m0.7685887377963737\u001b[0m, weights = \u001b[1m[0.15547828 0.         0.42174378 0.1333111  0.28946692]\u001b[0m\n",
      "[14:33:36] Blending: iteration \u001b[1m2\u001b[0m: score = \u001b[1m0.7686541143654114\u001b[0m, weights = \u001b[1m[0.15911226 0.         0.41696703 0.1336714  0.2902493 ]\u001b[0m\n",
      "[14:33:36] Blending: iteration \u001b[1m3\u001b[0m: score = \u001b[1m0.7686541143654114\u001b[0m, weights = \u001b[1m[0.15911226 0.         0.41696703 0.1336714  0.2902493 ]\u001b[0m\n",
      "[14:33:36] Blending: no score update. Terminated\n",
      "\n",
      "[14:33:36] \u001b[1mAutoml preset training completed in 24.55 seconds\u001b[0m\n",
      "\n",
      "[14:33:36] Model description:\n",
      "Final prediction for new objects (level 0) = \n",
      "\t 0.15911 * (5 averaged models Lvl_0_Pipe_0_Mod_0_LinearL2) +\n",
      "\t 0.41697 * (5 averaged models Lvl_0_Pipe_1_Mod_1_Tuned_LightGBM) +\n",
      "\t 0.13367 * (5 averaged models Lvl_0_Pipe_1_Mod_2_CatBoost) +\n",
      "\t 0.29025 * (5 averaged models Lvl_0_Pipe_1_Mod_3_Tuned_CatBoost) \n",
      "\n",
      "[14:33:36] ==================================================\n",
      "[14:33:36] Blending: optimization starts with equal weights and score \u001b[1m0.7755513423988842\u001b[0m\n",
      "[14:33:36] Blending: iteration \u001b[1m0\u001b[0m: score = \u001b[1m0.776477510460251\u001b[0m, weights = \u001b[1m[0.36067978 0.63932025]\u001b[0m\n",
      "[14:33:36] Blending: iteration \u001b[1m1\u001b[0m: score = \u001b[1m0.776477510460251\u001b[0m, weights = \u001b[1m[0.36067978 0.63932025]\u001b[0m\n",
      "[14:33:36] Blending: no score update. Terminated\n",
      "\n",
      "[2024-11-02 14:33:36,698] - [    END     ] - Fitting TabularLamaUtilized\n",
      "[2024-11-02 14:33:36,813] - [   SCORE    ] - Train: 0.8923683751743375\n",
      "[2024-11-02 14:33:36,816] - [   SCORE    ] - OOF: 0.776477510460251\n",
      "[2024-11-02 14:33:36,931] - [   SCORE    ] - Test: 0.7695792880258899\n",
      "[2024-11-02 14:33:36,932] - [   SCORE    ] - Overfit: 13.76 %\n",
      "[2024-11-02 14:33:37,074] - [    END     ] - Working with TabularLamaUtilized\n",
      "[2024-11-02 14:33:37,075] - [BEST  MODEL ] - TabularLama. Best score: 0.7724036481318035 \n",
      "\n",
      "[2024-11-02 14:33:37,076] - [   MODEL    ] - 9 out of 15. TabularLamaNN_mlp\n",
      "[2024-11-02 14:33:37,077] - [   START    ] - Working with TabularLamaNN_mlp\n",
      "[2024-11-02 14:33:37,078] - [   START    ] - Fitting TabularLamaNN_mlp\n",
      "[14:33:37] Stdout logging level is INFO.\n",
      "[14:33:37] Task: binary\n",
      "\n",
      "[14:33:37] Start automl preset with listed constraints:\n",
      "[14:33:37] - time: 60.00 seconds\n",
      "[14:33:37] - CPU: 12 cores\n",
      "[14:33:37] - memory: 16 GB\n",
      "\n",
      "[14:33:37] \u001b[1mTrain data shape: (623, 7)\u001b[0m\n",
      "\n",
      "[14:33:37] Layer \u001b[1m1\u001b[0m train process start. Time left 59.94 secs\n",
      "[14:33:37] Start fitting \u001b[1mLvl_0_Pipe_0_Mod_0_TorchNN_mlp_0\u001b[0m ...\n",
      "[14:33:42] Fitting \u001b[1mLvl_0_Pipe_0_Mod_0_TorchNN_mlp_0\u001b[0m finished. score = \u001b[1m0.6767346582984659\u001b[0m\n",
      "[14:33:42] \u001b[1mLvl_0_Pipe_0_Mod_0_TorchNN_mlp_0\u001b[0m fitting and predicting completed\n",
      "[14:33:42] Time left 54.73 secs\n",
      "\n",
      "[14:33:42] \u001b[1mLayer 1 training completed.\u001b[0m\n",
      "\n",
      "[14:33:42] \u001b[1mAutoml preset training completed in 5.27 seconds\u001b[0m\n",
      "\n",
      "[14:33:42] Model description:\n",
      "Final prediction for new objects (level 0) = \n",
      "\t 1.00000 * (5 averaged models Lvl_0_Pipe_0_Mod_0_TorchNN_mlp_0) \n",
      "\n",
      "[2024-11-02 14:33:42,374] - [    END     ] - Fitting TabularLamaNN_mlp\n",
      "[2024-11-02 14:33:43,796] - [   SCORE    ] - Train: 0.7151107043235705\n",
      "[2024-11-02 14:33:43,798] - [   SCORE    ] - OOF: 0.6767346582984659\n",
      "[2024-11-02 14:33:45,142] - [   SCORE    ] - Test: 0.6746101794645485\n",
      "[2024-11-02 14:33:45,143] - [   SCORE    ] - Overfit: 5.66 %\n",
      "[2024-11-02 14:33:45,260] - [    END     ] - Working with TabularLamaNN_mlp\n",
      "[2024-11-02 14:33:45,261] - [BEST  MODEL ] - TabularLama. Best score: 0.7724036481318035 \n",
      "\n",
      "[2024-11-02 14:33:45,262] - [   MODEL    ] - 10 out of 15. TabularLamaNN_denselight\n",
      "[2024-11-02 14:33:45,263] - [   START    ] - Working with TabularLamaNN_denselight\n",
      "[2024-11-02 14:33:45,264] - [   START    ] - Fitting TabularLamaNN_denselight\n",
      "[14:33:45] Stdout logging level is INFO.\n",
      "[14:33:45] Task: binary\n",
      "\n",
      "[14:33:45] Start automl preset with listed constraints:\n",
      "[14:33:45] - time: 60.00 seconds\n",
      "[14:33:45] - CPU: 12 cores\n",
      "[14:33:45] - memory: 16 GB\n",
      "\n",
      "[14:33:45] \u001b[1mTrain data shape: (623, 7)\u001b[0m\n",
      "\n",
      "[14:33:45] Layer \u001b[1m1\u001b[0m train process start. Time left 59.94 secs\n",
      "[14:33:45] Start fitting \u001b[1mLvl_0_Pipe_0_Mod_0_TorchNN_denselight_0\u001b[0m ...\n",
      "[14:33:50] Fitting \u001b[1mLvl_0_Pipe_0_Mod_0_TorchNN_denselight_0\u001b[0m finished. score = \u001b[1m0.6744028940027894\u001b[0m\n",
      "[14:33:50] \u001b[1mLvl_0_Pipe_0_Mod_0_TorchNN_denselight_0\u001b[0m fitting and predicting completed\n",
      "[14:33:50] Time left 54.88 secs\n",
      "\n",
      "[14:33:50] \u001b[1mLayer 1 training completed.\u001b[0m\n",
      "\n",
      "[14:33:50] \u001b[1mAutoml preset training completed in 5.12 seconds\u001b[0m\n",
      "\n",
      "[14:33:50] Model description:\n",
      "Final prediction for new objects (level 0) = \n",
      "\t 1.00000 * (5 averaged models Lvl_0_Pipe_0_Mod_0_TorchNN_denselight_0) \n",
      "\n",
      "[2024-11-02 14:33:50,414] - [    END     ] - Fitting TabularLamaNN_denselight\n",
      "[2024-11-02 14:33:51,782] - [   SCORE    ] - Train: 0.6941902022315201\n",
      "[2024-11-02 14:33:51,785] - [   SCORE    ] - OOF: 0.6744028940027894\n",
      "[2024-11-02 14:33:53,115] - [   SCORE    ] - Test: 0.636069432185937\n",
      "[2024-11-02 14:33:53,117] - [   SCORE    ] - Overfit: 8.37 %\n",
      "[2024-11-02 14:33:53,235] - [    END     ] - Working with TabularLamaNN_denselight\n",
      "[2024-11-02 14:33:53,236] - [BEST  MODEL ] - TabularLama. Best score: 0.7724036481318035 \n",
      "\n",
      "[2024-11-02 14:33:53,237] - [   MODEL    ] - 11 out of 15. TabularLamaNN_dense\n",
      "[2024-11-02 14:33:53,238] - [   START    ] - Working with TabularLamaNN_dense\n",
      "[2024-11-02 14:33:53,239] - [   START    ] - Fitting TabularLamaNN_dense\n",
      "[14:33:53] Stdout logging level is INFO.\n",
      "[14:33:53] Task: binary\n",
      "\n",
      "[14:33:53] Start automl preset with listed constraints:\n",
      "[14:33:53] - time: 60.00 seconds\n",
      "[14:33:53] - CPU: 12 cores\n",
      "[14:33:53] - memory: 16 GB\n",
      "\n",
      "[14:33:53] \u001b[1mTrain data shape: (623, 7)\u001b[0m\n",
      "\n",
      "[14:33:53] Layer \u001b[1m1\u001b[0m train process start. Time left 59.94 secs\n",
      "[14:33:53] Start fitting \u001b[1mLvl_0_Pipe_0_Mod_0_TorchNN_dense_0\u001b[0m ...\n",
      "[14:34:01] Fitting \u001b[1mLvl_0_Pipe_0_Mod_0_TorchNN_dense_0\u001b[0m finished. score = \u001b[1m0.6770506450488145\u001b[0m\n",
      "[14:34:01] \u001b[1mLvl_0_Pipe_0_Mod_0_TorchNN_dense_0\u001b[0m fitting and predicting completed\n",
      "[14:34:01] Time left 51.89 secs\n",
      "\n",
      "[14:34:01] \u001b[1mLayer 1 training completed.\u001b[0m\n",
      "\n",
      "[14:34:01] \u001b[1mAutoml preset training completed in 8.11 seconds\u001b[0m\n",
      "\n",
      "[14:34:01] Model description:\n",
      "Final prediction for new objects (level 0) = \n",
      "\t 1.00000 * (5 averaged models Lvl_0_Pipe_0_Mod_0_TorchNN_dense_0) \n",
      "\n",
      "[2024-11-02 14:34:01,382] - [    END     ] - Fitting TabularLamaNN_dense\n",
      "[2024-11-02 14:34:02,827] - [   SCORE    ] - Train: 0.7722280334728033\n",
      "[2024-11-02 14:34:02,829] - [   SCORE    ] - OOF: 0.6770506450488145\n",
      "[2024-11-02 14:34:04,249] - [   SCORE    ] - Test: 0.7066195939982348\n",
      "[2024-11-02 14:34:04,251] - [   SCORE    ] - Overfit: 8.50 %\n",
      "[2024-11-02 14:34:04,995] - [    END     ] - Working with TabularLamaNN_dense\n",
      "[2024-11-02 14:34:04,996] - [BEST  MODEL ] - TabularLama. Best score: 0.7724036481318035 \n",
      "\n",
      "[2024-11-02 14:34:04,997] - [   MODEL    ] - 12 out of 15. TabularLamaNN_resnet\n",
      "[2024-11-02 14:34:04,997] - [   START    ] - Working with TabularLamaNN_resnet\n",
      "[2024-11-02 14:34:04,998] - [   START    ] - Fitting TabularLamaNN_resnet\n",
      "[14:34:05] Stdout logging level is INFO.\n",
      "[14:34:05] Task: binary\n",
      "\n",
      "[14:34:05] Start automl preset with listed constraints:\n",
      "[14:34:05] - time: 60.00 seconds\n",
      "[14:34:05] - CPU: 12 cores\n",
      "[14:34:05] - memory: 16 GB\n",
      "\n",
      "[14:34:05] \u001b[1mTrain data shape: (623, 7)\u001b[0m\n",
      "\n",
      "[14:34:05] Layer \u001b[1m1\u001b[0m train process start. Time left 59.94 secs\n",
      "[14:34:05] Start fitting \u001b[1mLvl_0_Pipe_0_Mod_0_TorchNN_resnet_0\u001b[0m ...\n",
      "[14:34:09] Fitting \u001b[1mLvl_0_Pipe_0_Mod_0_TorchNN_resnet_0\u001b[0m finished. score = \u001b[1m0.6768872036262205\u001b[0m\n",
      "[14:34:09] \u001b[1mLvl_0_Pipe_0_Mod_0_TorchNN_resnet_0\u001b[0m fitting and predicting completed\n",
      "[14:34:09] Time left 55.90 secs\n",
      "\n",
      "[14:34:09] \u001b[1mLayer 1 training completed.\u001b[0m\n",
      "\n",
      "[14:34:09] \u001b[1mAutoml preset training completed in 4.10 seconds\u001b[0m\n",
      "\n",
      "[14:34:09] Model description:\n",
      "Final prediction for new objects (level 0) = \n",
      "\t 1.00000 * (5 averaged models Lvl_0_Pipe_0_Mod_0_TorchNN_resnet_0) \n",
      "\n",
      "[2024-11-02 14:34:09,129] - [    END     ] - Fitting TabularLamaNN_resnet\n",
      "[2024-11-02 14:34:10,481] - [   SCORE    ] - Train: 0.6938251830543933\n",
      "[2024-11-02 14:34:10,484] - [   SCORE    ] - OOF: 0.6768872036262205\n",
      "[2024-11-02 14:34:11,862] - [   SCORE    ] - Test: 0.6893203883495146\n",
      "[2024-11-02 14:34:11,863] - [   SCORE    ] - Overfit: 0.65 %\n",
      "[2024-11-02 14:34:11,937] - [    END     ] - Working with TabularLamaNN_resnet\n",
      "[2024-11-02 14:34:11,938] - [BEST  MODEL ] - TabularLama. Best score: 0.7724036481318035 \n",
      "\n",
      "[2024-11-02 14:34:11,940] - [   MODEL    ] - 13 out of 15. TabularLamaNN_node\n",
      "[2024-11-02 14:34:11,940] - [   START    ] - Working with TabularLamaNN_node\n",
      "[2024-11-02 14:34:11,942] - [   START    ] - Fitting TabularLamaNN_node\n",
      "[14:34:11] Stdout logging level is INFO.\n",
      "[14:34:11] Task: binary\n",
      "\n",
      "[14:34:11] Start automl preset with listed constraints:\n",
      "[14:34:11] - time: 60.00 seconds\n",
      "[14:34:11] - CPU: 12 cores\n",
      "[14:34:11] - memory: 16 GB\n",
      "\n",
      "[14:34:11] \u001b[1mTrain data shape: (623, 7)\u001b[0m\n",
      "\n",
      "[14:34:12] Layer \u001b[1m1\u001b[0m train process start. Time left 59.94 secs\n",
      "[14:34:12] Start fitting \u001b[1mLvl_0_Pipe_0_Mod_0_TorchNN_node_0\u001b[0m ...\n",
      "[14:35:02] Time limit exceeded after calculating fold 0\n",
      "\n",
      "[14:35:02] Fitting \u001b[1mLvl_0_Pipe_0_Mod_0_TorchNN_node_0\u001b[0m finished. score = \u001b[1m0.6496212121212122\u001b[0m\n",
      "[14:35:02] \u001b[1mLvl_0_Pipe_0_Mod_0_TorchNN_node_0\u001b[0m fitting and predicting completed\n",
      "[14:35:02] Time left 9.54 secs\n",
      "\n",
      "[14:35:02] Time limit exceeded in one of the tasks. AutoML will blend level 1 models.\n",
      "\n",
      "[14:35:02] \u001b[1mLayer 1 training completed.\u001b[0m\n",
      "\n",
      "[14:35:02] \u001b[1mAutoml preset training completed in 50.46 seconds\u001b[0m\n",
      "\n",
      "[14:35:02] Model description:\n",
      "Final prediction for new objects (level 0) = \n",
      "\t 1.00000 * (1 averaged models Lvl_0_Pipe_0_Mod_0_TorchNN_node_0) \n",
      "\n",
      "[2024-11-02 14:35:02,431] - [    END     ] - Fitting TabularLamaNN_node\n",
      "[2024-11-02 14:35:03,123] - [   SCORE    ] - Train: 0.7034845711297071\n",
      "[2024-11-02 14:35:03,126] - [   SCORE    ] - OOF: 0.6496212121212122\n",
      "[2024-11-02 14:35:03,576] - [   SCORE    ] - Test: 0.6463077375698735\n",
      "[2024-11-02 14:35:03,577] - [   SCORE    ] - Overfit: 8.13 %\n",
      "[2024-11-02 14:35:03,627] - [    END     ] - Working with TabularLamaNN_node\n",
      "[2024-11-02 14:35:03,628] - [BEST  MODEL ] - TabularLama. Best score: 0.7724036481318035 \n",
      "\n",
      "[2024-11-02 14:35:03,629] - [   MODEL    ] - 14 out of 15. TabularLamaNN_autoint\n",
      "[2024-11-02 14:35:03,630] - [   START    ] - Working with TabularLamaNN_autoint\n",
      "[2024-11-02 14:35:03,631] - [   START    ] - Fitting TabularLamaNN_autoint\n",
      "[14:35:03] Stdout logging level is INFO.\n",
      "[14:35:03] Task: binary\n",
      "\n",
      "[14:35:03] Start automl preset with listed constraints:\n",
      "[14:35:03] - time: 60.00 seconds\n",
      "[14:35:03] - CPU: 12 cores\n",
      "[14:35:03] - memory: 16 GB\n",
      "\n",
      "[14:35:03] \u001b[1mTrain data shape: (623, 7)\u001b[0m\n",
      "\n",
      "[14:35:03] Layer \u001b[1m1\u001b[0m train process start. Time left 59.94 secs\n",
      "[14:35:03] Start fitting \u001b[1mLvl_0_Pipe_0_Mod_0_TorchNN_autoint_0\u001b[0m ...\n",
      "[14:35:20] Fitting \u001b[1mLvl_0_Pipe_0_Mod_0_TorchNN_autoint_0\u001b[0m finished. score = \u001b[1m0.7223348152022314\u001b[0m\n",
      "[14:35:20] \u001b[1mLvl_0_Pipe_0_Mod_0_TorchNN_autoint_0\u001b[0m fitting and predicting completed\n",
      "[14:35:20] Time left 43.11 secs\n",
      "\n",
      "[14:35:20] \u001b[1mLayer 1 training completed.\u001b[0m\n",
      "\n",
      "[14:35:20] \u001b[1mAutoml preset training completed in 16.89 seconds\u001b[0m\n",
      "\n",
      "[14:35:20] Model description:\n",
      "Final prediction for new objects (level 0) = \n",
      "\t 1.00000 * (5 averaged models Lvl_0_Pipe_0_Mod_0_TorchNN_autoint_0) \n",
      "\n",
      "[2024-11-02 14:35:20,551] - [    END     ] - Fitting TabularLamaNN_autoint\n",
      "[2024-11-02 14:35:22,079] - [   SCORE    ] - Train: 0.7501525453277544\n",
      "[2024-11-02 14:35:22,082] - [   SCORE    ] - OOF: 0.7223348152022314\n",
      "[2024-11-02 14:35:23,509] - [   SCORE    ] - Test: 0.7016769638128861\n",
      "[2024-11-02 14:35:23,510] - [   SCORE    ] - Overfit: 6.46 %\n",
      "[2024-11-02 14:35:23,908] - [    END     ] - Working with TabularLamaNN_autoint\n",
      "[2024-11-02 14:35:23,909] - [BEST  MODEL ] - TabularLama. Best score: 0.7724036481318035 \n",
      "\n",
      "[2024-11-02 14:35:23,910] - [   MODEL    ] - 15 out of 15. TabularLamaNN_fttransformer\n",
      "[2024-11-02 14:35:23,911] - [   START    ] - Working with TabularLamaNN_fttransformer\n",
      "[2024-11-02 14:35:23,912] - [   START    ] - Fitting TabularLamaNN_fttransformer\n",
      "[14:35:23] Stdout logging level is INFO.\n",
      "[14:35:23] Task: binary\n",
      "\n",
      "[14:35:23] Start automl preset with listed constraints:\n",
      "[14:35:23] - time: 60.00 seconds\n",
      "[14:35:23] - CPU: 12 cores\n",
      "[14:35:23] - memory: 16 GB\n",
      "\n",
      "[14:35:23] \u001b[1mTrain data shape: (623, 7)\u001b[0m\n",
      "\n",
      "[14:35:24] Layer \u001b[1m1\u001b[0m train process start. Time left 59.94 secs\n",
      "[14:35:24] Start fitting \u001b[1mLvl_0_Pipe_0_Mod_0_TorchNN_fttransformer_0\u001b[0m ...\n",
      "[14:35:39] Fitting \u001b[1mLvl_0_Pipe_0_Mod_0_TorchNN_fttransformer_0\u001b[0m finished. score = \u001b[1m0.726616980474198\u001b[0m\n",
      "[14:35:39] \u001b[1mLvl_0_Pipe_0_Mod_0_TorchNN_fttransformer_0\u001b[0m fitting and predicting completed\n",
      "[14:35:39] Time left 44.48 secs\n",
      "\n",
      "[14:35:39] \u001b[1mLayer 1 training completed.\u001b[0m\n",
      "\n",
      "[14:35:39] \u001b[1mAutoml preset training completed in 15.52 seconds\u001b[0m\n",
      "\n",
      "[14:35:39] Model description:\n",
      "Final prediction for new objects (level 0) = \n",
      "\t 1.00000 * (5 averaged models Lvl_0_Pipe_0_Mod_0_TorchNN_fttransformer_0) \n",
      "\n",
      "[2024-11-02 14:35:39,465] - [    END     ] - Fitting TabularLamaNN_fttransformer\n",
      "[2024-11-02 14:35:40,963] - [   SCORE    ] - Train: 0.7441705892608088\n",
      "[2024-11-02 14:35:40,966] - [   SCORE    ] - OOF: 0.726616980474198\n",
      "[2024-11-02 14:35:42,384] - [   SCORE    ] - Test: 0.6906737275669315\n",
      "[2024-11-02 14:35:42,385] - [   SCORE    ] - Overfit: 7.19 %\n",
      "[2024-11-02 14:35:42,625] - [    END     ] - Working with TabularLamaNN_fttransformer\n",
      "[2024-11-02 14:35:42,627] - [BEST  MODEL ] - TabularLama. Best score: 0.7724036481318035 \n",
      "\n",
      "100%|##########| 1/1 [07:07<00:00, 427.71s/it]\n"
     ]
    }
   ],
   "source": [
    "from automl.model import AutoML\n",
    "from automl.metrics import RocAuc\n",
    "\n",
    "\n",
    "results = []\n",
    "for train, test, target_name, dataset_name in get_train_test_datasets(datasets[:1], datasets_path='./datasets', test_size=0.3, random_state=0):\n",
    "    metric = RocAuc()\n",
    "    start_time = time.perf_counter()\n",
    "    automl = AutoML(task='classification', n_jobs=12, metric=metric, tuning_timeout=30,)\n",
    "    automl = automl.fit(\n",
    "        train.drop(columns=[target_name,]), train[target_name].to_numpy(), \n",
    "        test.drop(columns=[target_name,]), test[target_name].to_numpy(),\n",
    "    )\n",
    "    train_time = time.perf_counter() - start_time\n",
    "    \n",
    "    test_predictions = automl.predict(test.drop(columns=[target_name,]))\n",
    "    predict_time = time.perf_counter() - start_time - train_time\n",
    "    all_time = train_time + predict_time\n",
    "    \n",
    "    test_score = metric(test[target_name], test_predictions)\n",
    "    results.append((dataset_name, test_score, automl.best_model.name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('yasserh/titanic-dataset', 0.7724036481318035, 'TabularLama')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
